{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "This notebook contains examples I'll run during the presentation to illustrate some points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Models that don't use weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "n_neighbors = 2\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we generate some linear data\n",
    "\n",
    "num_pts = 100\n",
    "X = np.linspace(-10, 10, num_pts)\n",
    "Y = 0.6 * X + np.random.normal(0, 2, num_pts)\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we solve the normal equations to find the weights of our line.\n",
    "psuedo_inv = 1 / (X.T @ X) * X.T\n",
    "w = psuedo_inv @ Y\n",
    "\n",
    "linex = np.linspace(-13, 13, 10)\n",
    "liney = linex * w\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(linex, liney, 'r')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's try some different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts = 100\n",
    "X = np.linspace(-2, 15, num_pts)\n",
    "Y = 1.3 * (X - 3) + np.random.normal(0, 2, num_pts)\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, here's the same code as before to generate the line.\n",
    "psuedo_inv = 1 / (X.T @ X) * X.T\n",
    "w = psuedo_inv @ Y\n",
    "\n",
    "linex = np.linspace(-5, 16, 10)\n",
    "liney = linex * w\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(linex, liney, 'r')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a one to every data point!\n",
    "X_add = np.vstack((X, np.ones(len(X)))).T\n",
    "print(X_add.shape)\n",
    "\n",
    "psuedo_inv =  np.linalg.inv(X_add.T @ X_add) @ X_add.T\n",
    "w = psuedo_inv @ Y\n",
    "\n",
    "linex = np.linspace(-5, 16, 10)\n",
    "# add dimension to linex\n",
    "linex = np.vstack((linex, np.ones(len(linex)))).T\n",
    "liney = linex @ w\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(linex[:,0], liney, 'r')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some quadratic data\n",
    "X = np.linspace(-2, 4, 100)\n",
    "Y = 0.3 * np.square(X) - 0.4 * X + 0.1  + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what happens if we try to fit a regular line without bias\n",
    "psuedo_inv = 1 / (X.T @ X) * X.T\n",
    "w = psuedo_inv @ Y\n",
    "\n",
    "linex = np.linspace(-2, 4, 10)\n",
    "liney = linex * w\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(linex, liney, 'r')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what happens when we fit a line with bias!\n",
    "# Add a one to every data point!\n",
    "X_add = np.vstack((X, np.ones(len(X)))).T\n",
    "print(X_add.shape)\n",
    "print(X_add[:4])\n",
    "\n",
    "psuedo_inv =  np.linalg.inv(X_add.T @ X_add) @ X_add.T\n",
    "w = psuedo_inv @ Y\n",
    "\n",
    "linex = np.linspace(-2, 4, 10)\n",
    "# add dimension to linex\n",
    "linex = np.vstack((linex, np.ones(len(linex)))).T\n",
    "liney = linex @ w\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(linex[:,0], liney, 'r')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try adding quadratic features to the data!\n",
    "X_quad = np.vstack((np.square(X), X, np.ones(len(X)))).T\n",
    "print(X_quad.shape)\n",
    "print(X_quad[:4])\n",
    "\n",
    "psuedo_inv =  np.linalg.inv(X_quad.T @ X_quad) @ X_quad.T\n",
    "w = psuedo_inv @ Y\n",
    "\n",
    "linex = np.linspace(-2, 4, 20)\n",
    "# add dimension to linex\n",
    "linex = np.vstack((np.square(linex), linex, np.ones(len(linex)))).T\n",
    "liney = linex @ w\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(linex[:,1], liney, 'r')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some classification data\n",
    "c1_x1, c1_x2 = np.random.multivariate_normal([-2.5,3], [[1, 0.3],[0.3, 1]], 100).T\n",
    "c2_x1, c2_x2 = np.random.multivariate_normal([1,1], [[2, 1],[1, 2]], 100).T\n",
    "\n",
    "plt.plot(c1_x1, c1_x2, 'x')\n",
    "plt.plot(c2_x1, c2_x2, 'o')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "c1_X = np.vstack((c1_x1, c1_x2)).T\n",
    "c2_X = np.vstack((c2_x1, c2_x2)).T\n",
    "X = np.concatenate((c1_X, c2_X))\n",
    "y = np.concatenate((np.zeros(100), np.ones(100)))\n",
    "\n",
    "# Shuffle the data\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "X = X[permutation, :]\n",
    "y = y[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for the logistic classifier\n",
    "def sigmoid(x): # note that sigmoid = logistic\n",
    "    return np.exp(x) / (1 + np.exp(x))\n",
    "\n",
    "# Performs one gradient update step on the weights\n",
    "def update(w, X, y, lr):\n",
    "    grad = - np.dot(X.T, y - sigmoid(np.dot(X, w)))\n",
    "    return w - lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Parameters ###\n",
    "lr = 0.01\n",
    "steps = 100\n",
    "\n",
    "# initialize weights\n",
    "w = np.random.normal(0, 0.1, X.shape[1])\n",
    "\n",
    "# Run gradient descent\n",
    "for _ in range(steps):\n",
    "    w = update(w, X, y, lr)\n",
    "\n",
    "# Now, extract the descision boundary!\n",
    "# 0.5 = s(Xw)\n",
    "# 1 + exp(-Xw) = 2\n",
    "# exp(-Xw) = 1\n",
    "#  Xw = 0\n",
    "# Solve for the equation of the line (X is now a vector)\n",
    "# x1 w1 + x2 w2 = 0\n",
    "# x2 = - w1 / w2 * x1\n",
    "\n",
    "linex = np.linspace(-0.5, 0.5, 20)\n",
    "liney = -w[0] / w[1] * linex\n",
    "\n",
    "plt.plot(c1_x1, c1_x2, 'x')\n",
    "plt.plot(c2_x1, c2_x2, 'o')\n",
    "plt.plot(linex, liney, 'r')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a bias kernel this time! WOHOOO\n",
    "X_add = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "print(X_add[:4])\n",
    "\n",
    "### Model Parameters ###\n",
    "lr = 0.01\n",
    "steps = 100\n",
    "\n",
    "# initialize weights\n",
    "w = np.random.normal(0, 0.1, X_add.shape[1])\n",
    "\n",
    "# Run gradient descent\n",
    "for _ in range(steps):\n",
    "    w = update(w, X_add, y, lr)\n",
    "\n",
    "# Now, extract the descision boundary!\n",
    "# 0.5 = s(Xw)\n",
    "# 1 + exp(-Xw) = 2\n",
    "# exp(-Xw) = 1\n",
    "#  Xw = 0\n",
    "# Solve for the equation of the line (X is now a vector)\n",
    "# x1 w1 + x2 w2 + 1 w3 = 0\n",
    "# x2 = - w1 / w2 * x1 -w3/w2\n",
    "\n",
    "linex = np.linspace(-4, 1.6, 20)\n",
    "liney = -w[0] / w[1] * linex - w[2]/w[1]\n",
    "\n",
    "plt.plot(c1_x1, c1_x2, 'x')\n",
    "plt.plot(c2_x1, c2_x2, 'o')\n",
    "plt.plot(linex, liney, 'r')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Batch vs. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_reg(X, y, w, low, up):\n",
    "    linex = np.linspace(low, up, 20)\n",
    "    liney = -w[0] / w[1] * linex - w[2]/w[1]\n",
    "\n",
    "    plt.plot(c1_x1, c1_x2, 'x')\n",
    "    plt.plot(c2_x1, c2_x2, 'o')\n",
    "    plt.plot(linex, liney, 'r')\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some classification data, this time a bit tougher\n",
    "c1_x1, c1_x2 = np.random.multivariate_normal([-1.5,1.2], [[1, 0.5],[0.5, 1]], 50).T\n",
    "c2_x1, c2_x2 = np.random.multivariate_normal([0.4,0.6], [[2, 1],[1, 2]], 50).T\n",
    "\n",
    "plt.plot(c1_x1, c1_x2, 'x')\n",
    "plt.plot(c2_x1, c2_x2, 'o')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "c1_X = np.vstack((c1_x1, c1_x2)).T\n",
    "c2_X = np.vstack((c2_x1, c2_x2)).T\n",
    "X = np.concatenate((c1_X, c2_X))\n",
    "y = np.concatenate((np.zeros(50), np.ones(50)))\n",
    "\n",
    "# Shuffle the data\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "X = X[permutation, :]\n",
    "y = y[permutation]\n",
    "\n",
    "# We're going to always use a bias\n",
    "X = np.hstack((X, np.ones((X.shape[0], 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Batch Gradient Descent ###\n",
    "lr = 0.01\n",
    "steps = 5\n",
    "low = -2\n",
    "up = 2\n",
    "# initialize weights\n",
    "w = np.random.normal(0, 0.1, X.shape[1])\n",
    "\n",
    "plot_logistic_reg(X, y, w, low, up)\n",
    "\n",
    "# Run gradient descent\n",
    "for _ in range(steps):\n",
    "    w = update(w, X, y, lr)\n",
    "    plot_logistic_reg(X, y, w, low, up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Stochastic Gradient Descent ###\n",
    "lr = 0.01\n",
    "steps = 200\n",
    "low = -2\n",
    "up = 2\n",
    "# initialize weights\n",
    "w = np.random.normal(0, 0.1, X.shape[1])\n",
    "\n",
    "plot_logistic_reg(X, y, w, low, up)\n",
    "\n",
    "# Run gradient descent\n",
    "for i in range(steps):\n",
    "    rand_index = int(np.random.rand() * X.shape[0])\n",
    "    data_point = np.expand_dims(X[rand_index, :], axis=0)\n",
    "    w = update(w, data_point, np.expand_dims(y[rand_index], axis=0), lr)\n",
    "    if i % 20 == 0:\n",
    "        plot_logistic_reg(X, y, w, low, up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
